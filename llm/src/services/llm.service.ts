import axios, { AxiosResponse } from 'axios';
import { ChatMessage } from '../types';

interface OllamaGenerateRequest {
  model: string;
  prompt: string;
  stream: boolean;
  options?: {
    temperature?: number;
    top_p?: number;
    num_predict?: number;
    stop?: string[];
  };
}

interface OllamaGenerateResponse {
  model: string;
  created_at: string;
  response: string;
  done: boolean;
  context?: number[];
  total_duration?: number;
  load_duration?: number;
  prompt_eval_count?: number;
  prompt_eval_duration?: number;
  eval_count?: number;
  eval_duration?: number;
}

export class LLMService {
  // Singleton instance
  private static instance: LLMService;
  private ollamaUrl: string;
  private model: string;
  private healthCheckCache: { healthy: boolean; lastCheck: number } = { healthy: false, lastCheck: 0 };
  private modelInitialized: boolean = false;
  private initializationInProgress: boolean = false;
  private initializationPromise: Promise<void> | null = null;
  private readonly CACHE_TTL = 10000; // 10 seconds
  private readonly REQUEST_TIMEOUT = parseInt(process.env.LLM_TIMEOUT || '70000');
  private readonly HEALTH_TIMEOUT = 10000;

  // Private constructor for Singleton pattern
  private constructor() {
    this.ollamaUrl = process.env.OLLAMA_URL || 'http://localhost:11434';
    this.model = process.env.OLLAMA_MODEL || 'llama3.2:3b';
    
    console.log(`üöÄ LLM Service initialized:`, {
      url: this.ollamaUrl,
      model: this.model,
      timeout: this.REQUEST_TIMEOUT
    });
  }

  // Get singleton instance
  public static getInstance(): LLMService {
    if (!LLMService.instance) {
      LLMService.instance = new LLMService();
    }
    return LLMService.instance;
  }

  // Initialize the service (wait for Ollama and model)
  public async initialize(): Promise<void> {
    if (this.modelInitialized) {
      return Promise.resolve();
    }

    if (this.initializationPromise) {
      return this.initializationPromise;
    }

    this.initializationPromise = this.initializeModel();
    return this.initializationPromise;
  }

  // Internal initialization process
  private async initializeModel(): Promise<void> {
    if (this.initializationInProgress) return;
    
    this.initializationInProgress = true;
    console.log(`üöÄ Initializing LLM service with model: ${this.model}`);

    try {
      await this.waitForOllama();
      await this.ensureModelExists();
      await this.warmUpModel();
      
      this.modelInitialized = true;
      console.log(`‚úÖ LLM service initialized successfully`);
      
    } catch (error: any) {
      console.error(`‚ùå LLM service initialization failed:`, error.message);
      throw error;
    } finally {
      this.initializationInProgress = false;
    }
  }

  // Wait for Ollama service to be ready
  private async waitForOllama(maxRetries: number = 30): Promise<void> {
    for (let i = 0; i < maxRetries; i++) {
      try {
        const response = await axios.get(`${this.ollamaUrl}/api/tags`, { 
          timeout: 5000,
          headers: { 'Content-Type': 'application/json' }
        });
        if (response.status === 200) {
          console.log(`‚úÖ Ollama is ready after ${i + 1} attempts`);
          return;
        }
      } catch (error) {
        console.log(`‚è≥ Waiting for Ollama... (${i + 1}/${maxRetries})`);
        await new Promise(resolve => setTimeout(resolve, 2000));
      }
    }
    throw new Error('Ollama not ready after maximum retries');
  }

  // Check if model exists, pull if needed
  private async ensureModelExists(): Promise<void> {
    try {
      const response = await axios.get(`${this.ollamaUrl}/api/tags`, {
        timeout: this.HEALTH_TIMEOUT
      });
      const models = response.data?.models || [];
      const modelExists = models.some((model: any) => {
        const modelName = model.name || '';
        return modelName === this.model || modelName.startsWith(this.model.split(':')[0]);
      });

      if (modelExists) {
        console.log(`‚úÖ Model ${this.model} already exists`);
        return;
      }

      console.log(`üì• Pulling model: ${this.model} (this may take several minutes...)`);
      
      const pullResponse = await axios.post(`${this.ollamaUrl}/api/pull`, {
        name: this.model,
        stream: false
      }, {
        timeout: 600000, // 10 minutes
        validateStatus: (status) => status < 500
      });

      if (pullResponse.status === 200) {
        console.log(`‚úÖ Model ${this.model} pulled successfully`);
      } else {
        throw new Error(`Failed to pull model: ${pullResponse.status}`);
      }
      
    } catch (error: any) {
      console.error(`‚ùå Error ensuring model exists:`, error.message);
      throw error;
    }
  }

  // Warm up model for faster first response
  private async warmUpModel(): Promise<void> {
    try {
      console.log(`üî• Warming up model ${this.model}...`);
      
      const warmupRequest: OllamaGenerateRequest = {
        model: this.model,
        prompt: 'Hello',
        stream: false,
        options: {
          num_predict: 5,
          temperature: 0.1
        }
      };

      const response = await axios.post(`${this.ollamaUrl}/api/generate`, warmupRequest, {
        timeout: 30000,
        headers: { 'Content-Type': 'application/json' }
      });

      if (response.status === 200 && response.data.response) {
        console.log(`‚úÖ Model warmed up successfully`);
      }
    } catch (error: any) {
      console.warn(`‚ö†Ô∏è Model warmup failed (non-critical):`, error.message);
    }
  }

  // Check service health with caching
  async checkHealth(): Promise<boolean> {
    const now = Date.now();
    
    // Return cached result if still valid
    if (now - this.healthCheckCache.lastCheck < this.CACHE_TTL) {
      return this.healthCheckCache.healthy;
    }

    try {
      const response = await axios.get(`${this.ollamaUrl}/api/tags`, {
        timeout: this.HEALTH_TIMEOUT,
        headers: { 'Content-Type': 'application/json' }
      });

      if (response.status !== 200) {
        throw new Error(`Ollama API returned status ${response.status}`);
      }

      const models = response.data?.models || [];
      const modelAvailable = models.some((model: any) => {
        const modelName = model.name || '';
        return modelName === this.model || modelName.startsWith(this.model.split(':')[0]);
      });

      const healthy = modelAvailable && this.modelInitialized;
      
      // Update cache
      this.healthCheckCache = { healthy, lastCheck: now };
      return healthy;

    } catch (error: any) {
      console.error(`‚ùå Health check failed:`, error.message);
      this.healthCheckCache = { healthy: false, lastCheck: now };
      return false;
    }
  }

  // Generate AI response (non-streaming)
  async generateResponse(messages: ChatMessage[]) {
    if (!this.modelInitialized) {
      throw new Error('LLM service not ready. Model initialization in progress.');
    }

    const startTime = Date.now();
    const requestId = Math.random().toString(36).substring(7);
    
    try {
      console.log(`ü§ñ [${requestId}] Starting generation with model: ${this.model}`);
      
      const prompt = this.buildSimplePrompt(messages);
      
      const requestPayload: OllamaGenerateRequest = {
        model: this.model,
        prompt: prompt,
        stream: false,
        options: {
          temperature: parseFloat(process.env.DEFAULT_TEMPERATURE || '0.7'),
          top_p: parseFloat(process.env.DEFAULT_TOP_P || '0.9'),
          num_predict: parseInt(process.env.DEFAULT_MAX_TOKENS || '200')
        }
      };

      console.log(`üöÄ [${requestId}] Sending request to Ollama`);

      const response: AxiosResponse<OllamaGenerateResponse> = await axios.post(
        `${this.ollamaUrl}/api/generate`,
        requestPayload,
        { 
          timeout: this.REQUEST_TIMEOUT,
          headers: {
            'Content-Type': 'application/json'
          },
          validateStatus: (status) => status < 500
        }
      );

      const responseTime = Date.now() - startTime;

      if (response.status !== 200) {
        throw new Error(`Ollama returned status ${response.status}: ${JSON.stringify(response.data)}`);
      }

      if (!response.data.response) {
        throw new Error('No response content from Ollama');
      }

      console.log(`‚úÖ [${requestId}] Success in ${responseTime}ms`);

      return {
        content: response.data.response,
        responseTime: responseTime
      };

    } catch (error: any) {
      const responseTime = Date.now() - startTime;
      
      console.error(`‚ùå [${requestId}] Failed after ${responseTime}ms:`, {
        error: error.message,
        code: error.code
      });

      if (error.code === 'ECONNABORTED') {
        throw new Error(`Request timed out after ${responseTime}ms`);
      }
      
      throw new Error(`LLM Error: ${error.message}`);
    }
  }

  // Build fitness coach prompt
  private buildSimplePrompt(messages: ChatMessage[]): string {
    const lastMessage = messages[messages.length - 1];
    
    if (!lastMessage) {
      return "Hello";
    }
    
    const prompt = `You are CoachGPT Pro, a fitness coach. Give helpful, concise advice (2-3 sentences).

    User: ${lastMessage.content}

    Coach:`;
    
    return prompt;
  }

  // Generate streaming response
  async generateStreamResponse(messages: ChatMessage[]) {
    if (!this.modelInitialized) {
      throw new Error('LLM service not ready. Model initialization in progress.');
    }
  
    const requestId = Math.random().toString(36).substring(7);
    
    try {
      console.log(`üåä [${requestId}] Starting streaming generation`);
      
      const prompt = this.buildSimplePrompt(messages);
      
      const response = await axios.post(`${this.ollamaUrl}/api/generate`, {
        model: this.model,
        prompt: prompt,
        stream: true,
        options: { 
          temperature: parseFloat(process.env.DEFAULT_TEMPERATURE || '0.7'),
          num_predict: parseInt(process.env.DEFAULT_MAX_TOKENS || '200')
        }
      }, { 
        responseType: 'stream', 
        timeout: this.REQUEST_TIMEOUT,
        headers: { 'Content-Type': 'application/json' }
      });
  
      return this.parseStreamResponse(response.data);
    } catch (error: any) {
      console.error(`‚ùå [${requestId}] Stream generation error:`, error.message);
      throw new Error(`LLM Stream Error: ${error.message}`);
    }
  }

  // Parse streaming chunks from Ollama
  private async* parseStreamResponse(stream: any) {
    let buffer = '';
    for await (const chunk of stream) {
      buffer += chunk.toString();
      const lines = buffer.split('\n');
      buffer = lines.pop() || '';
  
      for (const line of lines) {
        if (line.trim()) {
          try {
            const parsed = JSON.parse(line);
            if (parsed.response) {
              yield parsed.response;
            }
            if (parsed.done) return;
          } catch (e) {
            continue;
          }
        }
      }
    }
  }

  // Get list of available models
  async getAvailableModels(): Promise<string[]> {
    try {
      const response = await axios.get(`${this.ollamaUrl}/api/tags`, { 
        timeout: this.HEALTH_TIMEOUT 
      });
      return response.data.models?.map((model: any) => model.name) || [];
    } catch (error) {
      console.error('Failed to get available models:', error);
      return [];
    }
  }

  // Pull new model from Ollama
  async pullModel(modelName: string): Promise<boolean> {
    try {
      console.log(`üì• Pulling model: ${modelName}`);
      await axios.post(`${this.ollamaUrl}/api/pull`, {
        name: modelName,
        stream: false
      }, { timeout: 600000 });
      console.log(`‚úÖ Model ${modelName} pulled successfully`);
      return true;
    } catch (error) {
      console.error(`‚ùå Failed to pull model ${modelName}:`, error);
      return false;
    }
  }

  // Check if service is ready
  isReady(): boolean {
    return this.modelInitialized;
  }

  // Force reinitialize service
  async reinitialize(): Promise<void> {
    this.modelInitialized = false;
    this.initializationPromise = null;
    await this.initialize();
  }

  // Test generation capability
  async testGeneration(): Promise<void> {
    try {
      console.log('üß™ Running LLM service test...');
      
      const testMessages: ChatMessage[] = [
        { role: 'user', content: 'Hi' }
      ];

      const response = await this.generateResponse(testMessages);
      
      console.log('‚úÖ Test generation successful:', {
        responseLength: response.content.length,
        duration: response.responseTime
      });
      
    } catch (error: any) {
      console.error('‚ùå Test generation failed:', error.message);
      throw error;
    }
  }
}